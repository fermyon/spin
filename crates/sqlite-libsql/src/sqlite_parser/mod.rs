#![allow(unused_parens)] // for the generated files

// These modules are generated by ANTLR. To regenerate:
//
// (in some scratch directory)
//
// * wget https://github.com/rrevenantt/antlr4rust/releases/download/antlr4-4.8-2-Rust0.3.0-beta/antlr4-4.8-2-SNAPSHOT-complete.jar
// * git clone https://github.com/antlr/grammars-v4
// * java -jar ./antlr4-4.8-2-SNAPSHOT-complete.jar -Dlanguage=Rust ./grammars-v4/sql/sqlite/SQLiteLexer.g4
//
// then copy ./grammars-v4/sql/sqlite/*.rs to this directory.
//
// (You do _not_ need to generate the parser. The splitter uses only the lexer.)
//
// I realise this is manual and doesn't give much traceability back to the version of the grammar;
// it's worth considering how to improve that.

// Forget it, Jake. It's generated code.
#[allow(clippy::all)]
#[rustfmt::skip]
mod sqlitelexer;

use antlr_rust::common_token_stream::CommonTokenStream;
use antlr_rust::input_stream::InputStream;
use antlr_rust::int_stream::IntStream;
use antlr_rust::token_stream::TokenStream;

// This is ported as literally as I could from https://github.com/libsql/sqlite-antlr4-parser/blob/main/sqliteparserutils/utils.go,
// as that is a "linsql official" way to split a compound statement for the sqld batch
// protocol (see usage at https://github.com/libsql/libsql-client-go/blob/d21eb9b18da3d89c813c3dbf21b851bd01836c1c/libsql/internal/http/shared/statement.go#L40).
// The Rust looks like Go because, for ease of porting changes, it is *meant*
// to look like Go _grin_. The major changes from the Go implementation are:
//
// * current_token and previous_token are flattened to a (type, index) tuple. This
//   avoids conflict with the borrow checker.
// * atIncompleteMultilineCommentStart and atCreateTriggerStart are inlined into
//   the split function. This also avoids conflict with the borrow checker.
// * Token iteration is done inside the loop. This worked out easier for me than
//   trying to wedge things into the Rust `for` statement, though I'm sure it
//   could be done.
//
// A bit puzzlingly, the Go implementation cannot fail. I think this is because it only
// lexes the alleged SQL rather than truly parsing it. It leaves it up to the receiving
// parser to go "that's not legal SQL".
pub(crate) fn split_sql_statements(sql: &str) -> (Vec<String>, SplitStatementExtraInfo) {
    let lexer = sqlitelexer::SQLiteLexer::new(InputStream::new(sql));
    let mut token_stm = CommonTokenStream::new(lexer);

    let mut current_interval_start: isize = -1;
    let mut inside_create_trigger_stmt = false;
    let mut inside_multiline_comment = false;

    let mut previous_token = None;

    let mut intervals = vec![];

    loop {
        let (tok_type, tok_index) = {
            let current_token = token_stm.lt(1);
            match current_token {
                None => {
                    break;
                }
                Some(t) => {
                    if t.token_type == antlr_rust::token::TOKEN_EOF {
                        break;
                    } else {
                        (
                            t.token_type,
                            t.token_index.load(std::sync::atomic::Ordering::SeqCst),
                        )
                    }
                }
            }
        };

        let at_incomplete_multiline_comment_start = {
            // clippy: emulate flow of https://github.com/libsql/sqlite-antlr4-parser/blob/5cb5bb6044752852fb967a255b6ffc36fb1a6cd7/sqliteparserutils/utils.go#L105
            #[allow(clippy::needless_bool)]
            if token_stm.lt(1).get_token_type() != sqlitelexer::DIV {
                false
            } else if token_stm.lt(2).get_token_type() == sqlitelexer::STAR {
                true
            } else {
                false
            }
        };
        if at_incomplete_multiline_comment_start {
            inside_multiline_comment = true;
            break;
        }

        if current_interval_start == -1 {
            if tok_type == sqlitelexer::SCOL {
                previous_token = Some((tok_type, tok_index));
                token_stm.consume();
                continue;
            }

            current_interval_start = tok_index;

            let at_create_trigger_start = {
                // clippy: emulate flow of https://github.com/libsql/sqlite-antlr4-parser/blob/5cb5bb6044752852fb967a255b6ffc36fb1a6cd7/sqliteparserutils/utils.go#L85
                #[allow(clippy::if_same_then_else, clippy::needless_bool)]
                if token_stm.lt(1).get_token_type() != sqlitelexer::CREATE_ {
                    false
                } else if token_stm.lt(2).get_token_type() == sqlitelexer::TRIGGER_ {
                    true
                } else if token_stm.lt(2).get_token_type() == sqlitelexer::TEMP_
                    || token_stm.lt(2).get_token_type() == sqlitelexer::TEMPORARY_
                        && token_stm.lt(3).get_token_type() == sqlitelexer::TRIGGER_
                {
                    true
                } else {
                    false
                }
            };
            if at_create_trigger_start {
                inside_create_trigger_stmt = true;
                previous_token = Some((tok_type, tok_index));
                token_stm.consume();
                continue;
            }
        }

        if inside_create_trigger_stmt {
            if tok_type == sqlitelexer::END_ {
                inside_create_trigger_stmt = false;
            }
        } else if tok_type == sqlitelexer::SCOL {
            intervals.push((
                current_interval_start,
                previous_token.map(|(_, index)| index).unwrap_or(0),
            ));
            current_interval_start = -1;
        }

        previous_token = Some((tok_type, tok_index));
        token_stm.consume();
    }

    if current_interval_start != -1 {
        if let Some((_, prev_idx)) = previous_token {
            intervals.push((current_interval_start, prev_idx));
        }
    }

    let stmts: Vec<_> = intervals
        .into_iter()
        .map(|(start, stop)| token_stm.get_text_from_interval(start, stop))
        .collect();

    // The Go implementation does not consider these to be errors.  We'll
    // capture them to stay in line, but they're not used at the moment.
    let extra_info = SplitStatementExtraInfo {
        _incomplete_create_trigger_statement: inside_create_trigger_stmt,
        _incomplete_multiline_comment: inside_multiline_comment,
    };

    (stmts, extra_info)
}

pub(crate) struct SplitStatementExtraInfo {
    pub _incomplete_create_trigger_statement: bool,
    pub _incomplete_multiline_comment: bool,
}

// This exists to allow us to use an equivalent of Go's `GetTokenType` method, again
// just to keep the code reasonably aligned.
trait Token {
    fn get_token_type(&self) -> isize;
}

impl<'a> Token for Option<&Box<antlr_rust::token::GenericToken<std::borrow::Cow<'a, str>>>> {
    fn get_token_type(&self) -> isize {
        match self {
            None => antlr_rust::token::TOKEN_INVALID_TYPE,
            Some(t) => t.token_type,
        }
    }
}
